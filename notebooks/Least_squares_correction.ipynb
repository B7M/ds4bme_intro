{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Least squares correction",
      "provenance": [],
      "authorship_tag": "ABX9TyMy2ynmrp6kOLuGKXg266mN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bcaffo/ds4bme_intro/blob/master/notebooks/Least_squares_correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "558cC4d_R8kq",
        "colab_type": "text"
      },
      "source": [
        "# Least sqaures proof of minimization for regression through the origin\n",
        "\n",
        "Consider minimizing $\\sum_{i=1}^n (y_i - \\beta x_i)^2 = f(\\beta)$.\n",
        "\n",
        "The first derivative is\n",
        "$$\n",
        "f'(\\beta) = -2\\sum_{i=1}^n y_i x_i + 2\\beta \\sum_{i=1}^n x_i^2.\n",
        "$$\n",
        "\n",
        "Setting equal to 0, this yeilds the solution\n",
        "$$\n",
        "\\hat \\beta = \\frac{\\sum_{i=1}^n y_i x_i}{\\sum_{i=1}^n x_i^2}.\n",
        "$$\n",
        "\n",
        "The second derivative is\n",
        "$$\n",
        "f''(\\beta) = 2\\sum_{i=1}^n x_i^2 \n",
        "$$\n",
        "which is strictly positive provided any $x_i \\neq 0$. Thus, excepting the case where all $x_i$ are 0, $f$ is convex and so the root of our derivative is indeed a minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du5kEWjkTukl",
        "colab_type": "text"
      },
      "source": [
        "# (optional) More generic proof that doesn't require differentiation and works for $x$ and $y$ as functions or any other algebraic object\n",
        "\n",
        "Let $x$ an $y$ be objects that exist in a space with an inner product $<,>$ and norm, $||a||^2 = <a,a>$ that follow the standard rules (often such a space is called a Hilbert space). We show that $\\hat \\beta = <x,y>/<x,x>$ is the unique minimizer of  $||y - \\beta x||^2$ .  (Note this proof requires that you know the answer and isn't constructive, like the derivative proof when $x$ and $y$ are real vector).\n",
        "\n",
        "$$\n",
        "\\begin{align} \n",
        "||y - \\beta x||^2 & =  ||y - \\hat \\beta x + \\hat\\beta x - \\beta x||^2 \\\\\n",
        "& = ||y - \\hat \\beta x||^2 + 2<y-\\hat\\beta x, \\hat\\beta x - \\beta x> \n",
        "+ ||\\hat\\beta x - \\beta x||^2  \\\\\n",
        "& = ||y - \\hat \\beta x||^2 + 2<y,  x > \\hat \\beta - 2 <y,x> \\beta  + 2 <x, x> \\hat \\beta^2 + 2 <x, x>\\hat\\beta\\beta +   ||\\hat\\beta x - \\beta x||^2 \\\\\n",
        "& = ||y - \\hat \\beta x||^2 +\n",
        "2\\frac{<y,  x >^2}{||x||^2} - 2 <y, x> \\beta + 2\\frac{<y, x>^2}{||x||^2} + 2 <y, x> \\beta + ||\\hat\\beta x - \\beta x||^2 \\\\\n",
        "& = ||y - \\hat \\beta x||^2 + ||\\hat\\beta x - \\beta x||^2  \\\\\n",
        "& \\geq ||y - \\hat \\beta x||^2\n",
        "\\end{align}\n",
        "$$\n",
        "The inequality comes about since $||\\hat\\beta x - \\beta x||^2$ is necessarily non-negative so that dropping it can only make the quanitity smaller. Thus, \n",
        "for any $\\beta$, the quantity $||y - \\hat \\beta x||^2$ is larger than $||y - \\hat \\beta x||^2$ and so $\\hat \\beta$ must be the minimum.\n",
        "\n",
        "Note, I used some rules about inner products there.\n",
        "\n",
        "* $<a + b, c + d> = <a,c> + <a,d> + <b,c> + <b, d>$\n",
        "* $<c a, b> = <a, cb> = <a,b>c$ for constant $c$\n",
        "\n",
        "Example, let $<f, g> = \\int_0^1 f(t) g(t) dt$ for functions on $[0,1]$. Let $x = 1$ (i.e. the constant function) and let $y$ be any function. What's the best approximation of $y$ of the form $\\beta x$? According to our result, it has to be \n",
        "$$\n",
        "\\frac{<y, x>}{||x||^2} = <y, x> = \\int_0^1 y(t) dt.\n",
        "$$\n",
        "So, if you define $\\bar y = \\int_0^1 y(t) dt$ as the average, it's still the best approximation if you have no information."
      ]
    }
  ]
}